{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_metric_learning_exp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slala2121/Triplet-net-keras/blob/COS597D/deep_metric_learning_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaNjSYSpsPV4",
        "colab_type": "text"
      },
      "source": [
        "Code adapted from:\n",
        "https://github.com/KinWaiCheuk/Triplet-net-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Rx-qWmmlTw",
        "colab_type": "text"
      },
      "source": [
        "Other relevant links:\n",
        "\n",
        "scratch classification network from https://keras.io/examples/cifar10_resnet/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILvdYoh4ikGN",
        "colab_type": "code",
        "outputId": "2aea1c6e-c0d8-42b6-ae8f-96e87ee2205f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dOIE44amJSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dab07961-ff9d-4b36-f816-874811f23402"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "!pip install tensorflow-addons\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.0.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (1.12.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (2.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (3.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0->tensorflow-addons) (1.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (0.16.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (1.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (42.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0->tensorflow-addons) (2.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (4.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (3.0.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0->tensorflow-addons) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhYHleWkrVQ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9158d120-bbb7-48c8-c110-9b8a595e036a"
      },
      "source": [
        "# current work around for fixing the lifted structure loss file\n",
        "\n",
        "%%writefile /usr/local/lib/python3.6/dist-packages/tensorflow_addons/losses/lifted.py\n",
        "\n",
        "\n",
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Implements lifted_struct_loss.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow_addons.losses import metric_learning\n",
        "from tensorflow_addons.utils import keras_utils\n",
        "\n",
        "\n",
        "@keras_utils.register_keras_custom_object\n",
        "@tf.function\n",
        "def lifted_struct_loss(labels, embeddings, margin=1.0):\n",
        "    \"\"\"Computes the lifted structured loss.\n",
        "\n",
        "    Args:\n",
        "      labels: 1-D tf.int32 `Tensor` with shape [batch_size] of\n",
        "        multiclass integer labels.\n",
        "      embeddings: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        not be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "\n",
        "    Returns:\n",
        "      lifted_loss: tf.float32 scalar.\n",
        "    \"\"\"\n",
        "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
        "    lshape = tf.shape(labels)\n",
        "    # assert lshape.shape == 1\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix.\n",
        "    pairwise_distances = metric_learning.pairwise_distance(embeddings)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "\n",
        "    diff = margin - pairwise_distances\n",
        "    mask = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    # Safe maximum: Temporarily shift negative distances\n",
        "    #   above zero before taking max.\n",
        "    #     this is to take the max only among negatives.\n",
        "    row_minimums = tf.math.reduce_min(diff, 1, keepdims=True)\n",
        "    row_negative_maximums = tf.math.reduce_max(\n",
        "        tf.math.multiply(diff - row_minimums, mask), 1,\n",
        "        keepdims=True) + row_minimums\n",
        "\n",
        "    # Compute the loss.\n",
        "    # Keep track of matrix of maximums where M_ij = max(m_i, m_j)\n",
        "    #   where m_i is the max of alpha - negative D_i's.\n",
        "    # This matches the Caffe loss layer implementation at:\n",
        "    #   https://github.com/rksltnl/Caffe-Deep-Metric-Learning-CVPR16/blob/0efd7544a9846f58df923c8b992198ba5c355454/src/caffe/layers/lifted_struct_similarity_softmax_layer.cpp  # pylint: disable=line-too-long\n",
        "\n",
        "    max_elements = tf.math.maximum(row_negative_maximums,\n",
        "                                   tf.transpose(row_negative_maximums))\n",
        "    diff_tiled = tf.tile(diff, [batch_size, 1])\n",
        "    mask_tiled = tf.tile(mask, [batch_size, 1])\n",
        "    max_elements_vect = tf.reshape(tf.transpose(max_elements), [-1, 1])\n",
        "\n",
        "    loss_exp_left = tf.reshape(\n",
        "        tf.math.reduce_sum(\n",
        "            tf.math.multiply(\n",
        "                tf.math.exp(diff_tiled - max_elements_vect), mask_tiled),\n",
        "            1,\n",
        "            keepdims=True), [batch_size, batch_size])\n",
        "\n",
        "    loss_mat = max_elements + tf.math.log(loss_exp_left +\n",
        "                                          tf.transpose(loss_exp_left))\n",
        "    # Add the positive distance.\n",
        "    loss_mat += pairwise_distances\n",
        "\n",
        "    mask_positives = tf.cast(\n",
        "        adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
        "            tf.ones([batch_size]))\n",
        "\n",
        "    # *0.5 for upper triangular, and another *0.5 for 1/2 factor for loss^2.\n",
        "    num_positives = tf.math.reduce_sum(mask_positives) / 2.0\n",
        "\n",
        "    lifted_loss = tf.math.truediv(\n",
        "        0.25 * tf.math.reduce_sum(\n",
        "            tf.math.square(\n",
        "                tf.math.maximum(\n",
        "                    tf.math.multiply(loss_mat, mask_positives), 0.0))),\n",
        "        num_positives)\n",
        "    return lifted_loss\n",
        "\n",
        "\n",
        "@keras_utils.register_keras_custom_object\n",
        "class LiftedStructLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"Computes the lifted structured loss.\n",
        "\n",
        "    The loss encourages the positive distances (between a pair of embeddings\n",
        "    with the same labels) to be smaller than any negative distances (between\n",
        "    a pair of embeddings with different labels) in the mini-batch in a way\n",
        "    that is differentiable with respect to the embedding vectors.\n",
        "    See: https://arxiv.org/abs/1511.06452.\n",
        "\n",
        "    Args:\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      name: Optional name for the op.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=1.0, name=None):\n",
        "        super(LiftedStructLoss, self).__init__(\n",
        "            name=name, reduction=tf.keras.losses.Reduction.NONE)\n",
        "        self.margin = margin\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        return lifted_struct_loss(y_true, y_pred, self.margin)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"margin\": self.margin,\n",
        "        }\n",
        "        base_config = super(LiftedStructLoss, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.6/dist-packages/tensorflow_addons/losses/lifted.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErGQrxEs22B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u6gowOIiuxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from itertools import permutations\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "source_path=os.path.join('drive','My Drive', 'Colab Notebooks')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forZgD_0iJQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare dataset either for classification or deep metric learning\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset_name='cifar10'\n",
        "\n",
        "dataset_dir=os.path.join(source_path,dataset_name)\n",
        "if not os.path.isdir(dataset_dir):\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "debug=1\n",
        "\n",
        "if debug:\n",
        "  split_percent=1\n",
        "  train_split = tfds.Split.TRAIN.subsplit(tfds.percent[:split_percent])\n",
        "  test_split = tfds.Split.TEST.subsplit(tfds.percent[:split_percent])\n",
        "  train_dataset,info = tfds.load(name=dataset_name, split=train_split, as_supervised=True, with_info=True)\n",
        "  test_dataset,info = tfds.load(name=dataset_name, split=test_split, as_supervised=True, with_info=True)\n",
        "else:\n",
        "  train_dataset,info = tfds.load(name=dataset_name, split='train', as_supervised=True, with_info=True)\n",
        "  test_dataset,info = tfds.load(name=dataset_name, split='test', as_supervised=True, with_info=True)\n",
        "\n",
        "input_dim=info.features['image'].shape\n",
        "num_classes=info.features['label'].num_classes\n",
        "\n",
        "train_mean_path=os.path.join(dataset_dir,'train_mean.npy')\n",
        "if os.path.exists(train_mean_path):\n",
        "  train_mean=np.load(train_mean_path)\n",
        "else:\n",
        "  train_mean=[]\n",
        "  num_train_images=info.splits['train'].num_examples\n",
        "  train_mean=[]\n",
        "  for example in train_dataset.take(num_train_images):\n",
        "    image,label=example[0],example[1]\n",
        "    image=image.numpy().astype('float32')\n",
        "    if len(train_mean)==0:\n",
        "      train_mean=image\n",
        "    else:\n",
        "      train_mean = train_mean+image\n",
        "\n",
        "  train_mean=train_mean*1.0/num_train_images\n",
        "  np.save(train_mean_path,train_mean)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ug1PiOKC9EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare dataset either for classification or deep metric learning\n",
        "\n",
        "def _normalize_img(img, label):\n",
        "    img = img - train_mean\n",
        "    img = tf.cast(img, tf.float32) / 255.\n",
        "    return (img, label)\n",
        "\n",
        "\n",
        "# preprocessing of labels for classification\n",
        "\n",
        "def _encode_one_hot(img, label):\n",
        "    label = tf.one_hot(label,num_classes)\n",
        "    return (img, label)\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(_normalize_img)\n",
        "test_dataset = test_dataset.map(_normalize_img)\n",
        "\n",
        "loss_type='lifted'\n",
        "if loss_type=='classification':\n",
        "  train_dataset = train_dataset.map(_encode_one_hot)\n",
        "  test_dataset = test_dataset.map(_encode_one_hot)\n",
        "\n",
        "# Build your input pipelines\n",
        "\n",
        "train_dataset = train_dataset.shuffle(1024).batch(32)\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk_djf8anBK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "\n",
        "# based on the lifted scheme paper\n",
        "def create_deep_base_network(input_dim,loss_type,num_classes=0,transfer=False,freeze_weights=False):\n",
        "  weights='imagenet' if transfer else None\n",
        "  conv_base = ResNet50(weights=weights, include_top=False, input_shape=input_dim)\n",
        "\n",
        "  if freeze_weights:\n",
        "    for layer in conv_base.layers:\n",
        "      layer.trainable=False\n",
        "  \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Input(input_dim))\n",
        "  model.add(conv_base)\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.BatchNormalization())\n",
        "\n",
        "  if loss_type=='classification':\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "  else:\n",
        "    model.add(layers.Dense(64, activation=None))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def create_shallow_network(input_dim,loss_type,num_classes=0):\n",
        "  model=tf.keras.Sequential()\n",
        "  model.add(layers.Input(input_dim))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=2, padding='same'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if loss_type=='classification':\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers(Dense(num_classes,activation='softmax')))\n",
        "  else:\n",
        "    model.add(tf.keras.layers.Dense(256, activation=None))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  return model\n",
        "\n",
        "def construct_model(model_type,input_dim,loss_type,num_classes,transfer=False,freeze_weights=False):\n",
        "  if model_type=='shallow':\n",
        "    model=create_shallow_network(input_dim,loss_type,num_classes)\n",
        "  elif model_type=='deep':\n",
        "    model=create_deep_base_network(input_dim,loss_type,num_classes,transfer,freeze_weights)\n",
        "\n",
        "  if loss_type =='triplet':\n",
        "    model.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)))\n",
        "  return model\n",
        "\n",
        "def construct_loss(loss_type,margin):\n",
        "  if loss_type=='triplet':\n",
        "    loss=tfa.losses.TripletSemiHardLoss(margin=margin)\n",
        "  elif loss_type=='lifted':\n",
        "    loss=tfa.losses.LiftedStructLoss(margin=margin)\n",
        "  elif loss_type=='classification':\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy()\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlxiWw0svGrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af54ed18-0323-4185-cbeb-413949420256"
      },
      "source": [
        "# tune lr\n",
        "\n",
        "lrs=[1e-1,1e-2,1e-3,1e-4,1e-5]\n",
        "lrs=[1e-4]\n",
        "model_type='deep'\n",
        "transfer=True\n",
        "freeze_weights=False\n",
        "\n",
        "margin=1.0\n",
        "\n",
        "model_dir=os.path.join(dataset_dir,model_type)\n",
        "if not os.path.isdir(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "\n",
        "num_epochs=30\n",
        "\n",
        "fig,ax=plt.subplots(2,3)\n",
        "ax=ax.ravel()\n",
        "for lr_index,lr in enumerate(lrs):\n",
        "\n",
        "  model=construct_model(model_type,input_dim,loss_type,num_classes,transfer,freeze_weights)\n",
        "  loss=construct_loss(loss_type,margin)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss=loss)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      epochs=num_epochs)\n",
        "\n",
        "  ax[lr_index].set_title('Loss for lr_%s'%(str(lr)))\n",
        "  ax[lr_index].plot(np.arange(num_epochs),history.history['loss'],'r',label='train_loss_lr_%s'%(str(lr)))\n",
        "  \n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(model_dir,'loss_plot_lr_tune_%s_transfer_%s_freeze_%s_%s_%s.png'%(model_type,transfer,freeze_weights,loss_type,margin)))\n",
        "plt.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - 11s 706ms/step - loss: 39.6720\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 36.2597\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 32.1725\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 31.2699\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 29.6449\n",
            "Epoch 6/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 28.6109\n",
            "Epoch 7/30\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 27.3904\n",
            "Epoch 8/30\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 27.8023\n",
            "Epoch 9/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 28.2717\n",
            "Epoch 10/30\n",
            "16/16 [==============================] - 2s 116ms/step - loss: 26.7183\n",
            "Epoch 11/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 26.5799\n",
            "Epoch 12/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 25.9843\n",
            "Epoch 13/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 25.8457\n",
            "Epoch 14/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 27.2899\n",
            "Epoch 15/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 27.3383\n",
            "Epoch 16/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 26.1162\n",
            "Epoch 17/30\n",
            "16/16 [==============================] - 2s 112ms/step - loss: 25.3923\n",
            "Epoch 18/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 24.3040\n",
            "Epoch 19/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 25.7464\n",
            "Epoch 20/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 24.9450\n",
            "Epoch 21/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 25.4691\n",
            "Epoch 22/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 25.2120\n",
            "Epoch 23/30\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 24.0935\n",
            "Epoch 24/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 24.6871\n",
            "Epoch 25/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 24.2127\n",
            "Epoch 26/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 23.7373\n",
            "Epoch 27/30\n",
            "16/16 [==============================] - 2s 114ms/step - loss: 23.8144\n",
            "Epoch 28/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 24.0686\n",
            "Epoch 29/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 22.6466\n",
            "Epoch 30/30\n",
            "16/16 [==============================] - 2s 113ms/step - loss: 23.1413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E19n3qkFVyc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40ab1478-24f0-4122-ab3e-e773fa807bc6"
      },
      "source": [
        "# train model\n",
        "\n",
        "lr=1e-4\n",
        "\n",
        "model_type='deep'\n",
        "transfer=True\n",
        "freeze_weights=False\n",
        "\n",
        "margin=1.0\n",
        "\n",
        "model_dir=os.path.join(dataset_dir,model_type)\n",
        "if not os.path.isdir(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "\n",
        "num_epochs=30\n",
        "\n",
        "model=construct_model(model_type,input_dim,loss_type,num_classes,transfer,freeze_weights)\n",
        "loss=construct_loss(loss_type,margin)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss=loss)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "filepath=os.path.join(model_dir,'final_%s_%s_transfer_%s_freeze_%s_margin_%s.h5'%(loss_type,model_type,transfer,freeze_weights,margin))\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=callbacks)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(num_epochs),history.history['loss'],'r',label='train_loss')\n",
        "plt.plot(np.arange(num_epochs),history.history['val_loss'],'b',label='val_loss')\n",
        "plt.legend()\n",
        "filepath=os.path.join(model_dir,'final_loss_plot_%s_%s_transfer_%s_freeze_%s_margin_%s.png'%(loss_type,model_type,transfer,freeze_weights,margin))\n",
        "plt.savefig(filepath)\n",
        "plt.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "   1563/Unknown - 75s 48ms/step - loss: 1.7059\n",
            "Epoch 00001: val_loss improved from inf to 1.00551, saving model to drive/My Drive/Colab Notebooks/cifar10/deep/final_classification_deep_transfer_True_freeze_False_margin_1.0.h5\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 1.7059 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 1.1943\n",
            "Epoch 00002: val_loss improved from 1.00551 to 0.79665, saving model to drive/My Drive/Colab Notebooks/cifar10/deep/final_classification_deep_transfer_True_freeze_False_margin_1.0.h5\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 1.0900 - val_loss: 0.7967\n",
            "Epoch 3/30\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.8983\n",
            "Epoch 00003: val_loss improved from 0.79665 to 0.69601, saving model to drive/My Drive/Colab Notebooks/cifar10/deep/final_classification_deep_transfer_True_freeze_False_margin_1.0.h5\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.8504 - val_loss: 0.6960\n",
            "Epoch 4/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.7386\n",
            "Epoch 00004: val_loss improved from 0.69601 to 0.64233, saving model to drive/My Drive/Colab Notebooks/cifar10/deep/final_classification_deep_transfer_True_freeze_False_margin_1.0.h5\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.7046 - val_loss: 0.6423\n",
            "Epoch 5/30\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.6307\n",
            "Epoch 00005: val_loss improved from 0.64233 to 0.62354, saving model to drive/My Drive/Colab Notebooks/cifar10/deep/final_classification_deep_transfer_True_freeze_False_margin_1.0.h5\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.5879 - val_loss: 0.6235\n",
            "Epoch 6/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.5127\n",
            "Epoch 00006: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.4767 - val_loss: 0.6530\n",
            "Epoch 7/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.4096\n",
            "Epoch 00007: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.3868 - val_loss: 0.6274\n",
            "Epoch 8/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3417\n",
            "Epoch 00008: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.3163 - val_loss: 0.6487\n",
            "Epoch 9/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.2687\n",
            "Epoch 00009: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.2599 - val_loss: 0.7054\n",
            "Epoch 10/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.2305\n",
            "Epoch 00010: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.2180 - val_loss: 0.6847\n",
            "Epoch 11/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1970\n",
            "Epoch 00011: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.1908 - val_loss: 0.7255\n",
            "Epoch 12/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1686\n",
            "Epoch 00012: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1633 - val_loss: 0.7507\n",
            "Epoch 13/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1665\n",
            "Epoch 00013: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1544 - val_loss: 0.7086\n",
            "Epoch 14/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1463\n",
            "Epoch 00014: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1391 - val_loss: 0.7569\n",
            "Epoch 15/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1237\n",
            "Epoch 00015: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1224 - val_loss: 0.7586\n",
            "Epoch 16/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1359\n",
            "Epoch 00016: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.1212 - val_loss: 0.7734\n",
            "Epoch 17/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1207\n",
            "Epoch 00017: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.1124 - val_loss: 0.7722\n",
            "Epoch 18/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0974\n",
            "Epoch 00018: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.1011 - val_loss: 0.8161\n",
            "Epoch 19/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0856\n",
            "Epoch 00019: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0896 - val_loss: 0.8847\n",
            "Epoch 20/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.1023\n",
            "Epoch 00020: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0954 - val_loss: 0.8375\n",
            "Epoch 21/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0819\n",
            "Epoch 00021: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0837 - val_loss: 0.8832\n",
            "Epoch 22/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0804\n",
            "Epoch 00022: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0808 - val_loss: 0.9231\n",
            "Epoch 23/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0866\n",
            "Epoch 00023: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0844 - val_loss: 0.8137\n",
            "Epoch 24/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0730\n",
            "Epoch 00024: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0713 - val_loss: 0.8520\n",
            "Epoch 25/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0832\n",
            "Epoch 00025: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0763 - val_loss: 0.8856\n",
            "Epoch 26/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0612\n",
            "Epoch 00026: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0695 - val_loss: 0.8847\n",
            "Epoch 27/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0641\n",
            "Epoch 00027: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0679 - val_loss: 0.8424\n",
            "Epoch 28/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0537\n",
            "Epoch 00028: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0592 - val_loss: 0.8276\n",
            "Epoch 29/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0534\n",
            "Epoch 00029: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0640 - val_loss: 0.9205\n",
            "Epoch 30/30\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.0685\n",
            "Epoch 00030: val_loss did not improve from 0.62354\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0624 - val_loss: 0.9542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLn7LgjzSF9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d0588c2-1b96-453e-c35b-49a1dfa13c0a"
      },
      "source": [
        "# tune margin\n",
        "\n",
        "lr=1e-3\n",
        "margins=[0.2,1]\n",
        "\n",
        "model_type='deep'\n",
        "classification=False\n",
        "transfer=True\n",
        "freeze_weights=False\n",
        "\n",
        "loss_type='triplet'\n",
        "l2_normalize=1 if loss_type=='triplet' else 0\n",
        "\n",
        "model_dir=os.path.join(dataset_dir,model_type)\n",
        "if not os.path.isdir(model_dir):\n",
        "  os.mkdir(model_dir)\n",
        "\n",
        "num_epochs=25\n",
        "\n",
        "fig,ax=plt.subplots(1,len(margins))\n",
        "ax=ax.ravel()\n",
        "for margin_index,margin in enumerate(margins):\n",
        "\n",
        "  model=construct_model(model_type,input_dim,classification,num_classes,transfer,freeze_weights,l2_normalize)\n",
        "  loss=construct_loss(loss_type,margin)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss=loss)\n",
        "\n",
        "  history = model.fit(\n",
        "      train_dataset,\n",
        "      epochs=num_epochs)\n",
        "\n",
        "  ax[margin_index].set_title('Loss for margin_%s'%(str(margin)))\n",
        "  ax[margin_index].plot(np.arange(num_epochs),history.history['loss'],'r',label='train_loss_margin_%s'%(str(margin)))\n",
        "  \n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(model_dir,'loss_plot_margin_%s_transfer_%s_freeze_%s_%s_%s.png'%(model_type,transfer,freeze_weights,loss_type,margin)))\n",
        "plt.close()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - 10s 650ms/step - loss: 0.1618\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.1519\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1404\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1343\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.1330\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1303\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - 2s 124ms/step - loss: 0.1263\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1279\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1290\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1274\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1227\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - 2s 126ms/step - loss: 0.1183\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - 2s 128ms/step - loss: 0.1209\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.1200\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - 2s 124ms/step - loss: 0.1228\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1162\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1152\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - 2s 127ms/step - loss: 0.1146\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - 2s 125ms/step - loss: 0.1107\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.1156\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - 2s 122ms/step - loss: 0.1164\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - 2s 124ms/step - loss: 0.1087\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.1117\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - 2s 122ms/step - loss: 0.1084\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.1135\n",
            "Epoch 1/25\n",
            "16/16 [==============================] - 9s 579ms/step - loss: 0.9480\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - 2s 115ms/step - loss: 0.9404\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.9229\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9208\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 0.9208\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9198\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.9170\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.9177\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9113\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.9303\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 0.9271\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 0.9156\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - 2s 123ms/step - loss: 0.9046\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 0.9182\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - 2s 122ms/step - loss: 0.9131\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 0.9252\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - 2s 120ms/step - loss: 0.9126\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 0.9178\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9176\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 0.9026\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9089\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - 2s 121ms/step - loss: 0.9077\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - 2s 118ms/step - loss: 0.9032\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - 2s 119ms/step - loss: 0.9000\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - 2s 117ms/step - loss: 0.9088\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}